{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8e1396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns                       #visualisation\n",
    "import matplotlib.pyplot as plt             #visualisation\n",
    "# %matplotlib inline     \n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import random\n",
    "\n",
    "# import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68921923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logic\n",
    "# start the game\n",
    "# play then get reward of 4 and roll dice, if dice value is 1 or 2 then quit else continue the game\n",
    "# or quit and get reward of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3425c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TramMDP:\n",
    "    def __init__(self,N):\n",
    "        self.N = N  # Number of states\n",
    "        self.states = list(range(1, N+1))\n",
    "        # self.actions = ['walk', 'tram']\n",
    "        self.current_state = 1\n",
    "        self.endState = N\n",
    "        self.gamma = 1.0\n",
    "        self.total_reward = 0\n",
    "\n",
    "\n",
    "        # Define the transition probabilities and rewards as a combined dictionary\n",
    "        self.transitions = self._create_transitions()\n",
    "\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Return a list of valid actions for the given state.\"\"\"\n",
    "        if state == self.endState:\n",
    "            return [None]  # No actions are possible in the end state\n",
    "\n",
    "        possible_actions = []\n",
    "        if state + 1 <= self.N:  # If the next state is within the range, add 'walk'\n",
    "            possible_actions.append(\"walk\")\n",
    "        if state * 2 <= self.N:  # If the double of the state is within the range, add 'tram'\n",
    "            possible_actions.append(\"tram\")\n",
    "        return possible_actions\n",
    "\n",
    "\n",
    "\n",
    "# create transition and reward matrices\n",
    "        \n",
    "    # def _create_transitions(self):\n",
    "    #         transitions = {}\n",
    "    #         for state in self.states[:-1]:  # Exclude the 'game over' state\n",
    "    #             transitions[state] = {\n",
    "    #                 'walk': [((state + 1, 1.0, -1.0),)],\n",
    "    #                 'tram': [((min(state * 2, self.N), 0.7, -2.0),), ((state, 0.3, -2.0),)]\n",
    "    #             }\n",
    "    #         return transitions\n",
    "    def _create_transitions(self):\n",
    "        transitions = {\n",
    "            state: {\n",
    "                'walk': [(state + 1, 1.0, -1.0)],\n",
    "                'tram': [(min(state * 2, self.N), 0.5, -2.0), (state, 0.5, -2.0)]\n",
    "            } for state in self.states[:-1]  # Exclude the 'game over' state\n",
    "        }\n",
    "        return transitions\n",
    "\n",
    "    \n",
    "    def getTransition(self, state, action):\n",
    "        return self.transitions[state][action]\n",
    "\n",
    "\n",
    "    def take_action(self, action):\n",
    "        if self.current_state == self.endState:\n",
    "            return self.endState\n",
    "\n",
    "        transitions = self.getTransition(self.current_state, action)\n",
    "        if not transitions:\n",
    "            return self.current_state\n",
    "\n",
    "        # Extract the next states, probabilities, and rewards\n",
    "        next_states, probabilities, rewards = zip(*transitions)\n",
    "        \n",
    "        # Choose the next state based on the transition probabilities\n",
    "        next_state_index = random.choices(range(len(next_states)), weights=probabilities, k=1)[0]\n",
    "        next_state = next_states[next_state_index]\n",
    "        reward = rewards[next_state_index]\n",
    "\n",
    "        # Update current state and total reward\n",
    "        self.current_state = next_state\n",
    "        self.total_reward += reward\n",
    "\n",
    "        return self.current_state, reward\n",
    "\n",
    "\n",
    "\n",
    "    # def take_action(self, action):\n",
    "    #     if self.current_state == self.endState:\n",
    "    #         return self.endState\n",
    "\n",
    "    #     transitions = self.getTransition(self.current_state, action)\n",
    "    #     if not transitions:\n",
    "    #         return self.current_state\n",
    "\n",
    "    #     # Extract the next states and their probabilities\n",
    "    #     next_states, probabilities, rewards = zip(*[trans[0] for trans in transitions])\n",
    "        \n",
    "    #     # Choose the next state based on the transition probabilities\n",
    "    #     next_state_index = random.choices(range(len(next_states)), weights=probabilities, k=1)[0]\n",
    "    #     next_state = next_states[next_state_index]\n",
    "    #     reward = rewards[next_state_index]\n",
    "\n",
    "    #     # Update current state and total reward\n",
    "    #     self.current_state = next_state\n",
    "    #     self.total_reward += reward\n",
    "\n",
    "    #     return self.current_state, reward\n",
    "    \n",
    "    def playGame(self, policy):\n",
    "        self.reset()\n",
    "        while not self.isEnd(self.current_state):\n",
    "            action = policy[self.current_state]\n",
    "            self.take_action(action)\n",
    "        return self.total_reward\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = 1\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def getRandomPolicy(self):\n",
    "                # Assuming tram is an instance of your MDP class\n",
    "        policy = {}\n",
    "        for s in self.states:\n",
    "            available_actions = self.actions(s)\n",
    "            if available_actions:  # Check if there are available actions\n",
    "                policy[s] = random.choice(available_actions)\n",
    "            else:\n",
    "                policy[s] = None  # Assign None or a placeholder if no actions are available\n",
    "        return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f6ec527b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.5, -2.0), (1, 0.5, -2.0)]\n",
      "(1, -2.0)\n",
      "[(2, 0.5, -2.0), (1, 0.5, -2.0)]\n"
     ]
    }
   ],
   "source": [
    "tram = TramMDP(5)\n",
    "\n",
    "print(tram.getTransition(1, 'tram'))\n",
    "\n",
    "print(tram.take_action('tram'))\n",
    "\n",
    "print(tram.getTransition(1, 'tram'))\n",
    "\n",
    "\n",
    "# print(tram2.take_action_prob('tram'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "03193a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, mdp, theta=0.01):\n",
    "    # Initialize value function for each state\n",
    "    V_pi = {s: 0 for s in mdp.states if s != mdp.endState}\n",
    "\n",
    "    def is_end(s):\n",
    "        return s == mdp.endState\n",
    "\n",
    "    # Perform policy evaluation\n",
    "    while True:\n",
    "        delta = 0  # This will track the maximum change in the value function\n",
    "\n",
    "        for s in mdp.states:\n",
    "            if is_end(s):\n",
    "                V_pi[s] = 0\n",
    "            else:\n",
    "                # Calculate the action-value function Q for the current state and action according to policy\n",
    "                a = policy[s]\n",
    "                Q_pi = sum(prob * (reward + mdp.gamma * V_pi.get(s_prime, 0))\n",
    "                           for (s_prime, prob, reward) in mdp.getTransition(s, a))\n",
    "                \n",
    "                # Calculate the maximum change in the value function\n",
    "                delta = max(delta, abs(Q_pi - V_pi.get(s, 0)))\n",
    "                \n",
    "                # Update the state-value function V\n",
    "                V_pi[s] = Q_pi\n",
    "\n",
    "        # Check for convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Output the final state-value function\n",
    "    for s in mdp.states:\n",
    "        print(f\"Value of state {s}: {V_pi.get(s, 0)}\")\n",
    "\n",
    "    return V_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "657e0d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'walk', 2: 'tram', 3: 'walk', 4: 'walk', 5: None}\n",
      "Value of state 1: -5.994140625\n",
      "Value of state 2: -4.9970703125\n",
      "Value of state 3: -2.0\n",
      "Value of state 4: -1.0\n",
      "Value of state 5: 0\n",
      "{1: -5.994140625, 2: -4.9970703125, 3: -2.0, 4: -1.0, 5: 0}\n"
     ]
    }
   ],
   "source": [
    "policy = tram.getRandomPolicy()\n",
    "\n",
    "print(policy)\n",
    "\n",
    "print(policy_evaluation(policy,tram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f50b6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_with_policy(mdp, theta=1e-10):\n",
    "    # Initialize value function for each state\n",
    "    V = {s: 0 for s in mdp.states if s != mdp.endState}\n",
    "\n",
    "    def is_end(s):\n",
    "        \"\"\"Check if the current state is the end of the game.\"\"\"\n",
    "        return s == mdp.endState\n",
    "\n",
    "    def compute_Q(s, a, V):\n",
    "        \"\"\"Compute the action-value function Q for a given state and action.\"\"\"\n",
    "        return sum(prob * (reward + mdp.gamma * V.get(s_prime, 0))\n",
    "                   for s_prime, prob, reward in mdp.getTransition(s, a))\n",
    "\n",
    "    # Perform value iteration\n",
    "    while True:\n",
    "        delta = 0  # This will track the maximum change in the value function\n",
    "        for s in mdp.states:\n",
    "            if is_end(s):\n",
    "                V[s] = 0\n",
    "            else:\n",
    "                # Find the best action by comparing the Q value for each action\n",
    "                max_value = max(compute_Q(s, a, V) for a in mdp.actions(s))\n",
    "                delta = max(delta, abs(max_value - V.get(s, 0)))\n",
    "                V[s] = max_value\n",
    "\n",
    "        # Check for convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Read out policy\n",
    "    pi = {}\n",
    "    for state in mdp.states:\n",
    "        if is_end(state):\n",
    "            pi[state] = 'none'\n",
    "        else:\n",
    "            pi[state] = max(\n",
    "                ((compute_Q(state, action, V), action) for action in mdp.actions(state)), \n",
    "                key=lambda x: x[0]\n",
    "            )[1]  # Select the action from the (value, action) tuple\n",
    "\n",
    "    # Print out the policy and values\n",
    "    print('{:15} {:15} {:15}'.format('s', 'V(s)', 'pi(s)'))\n",
    "    for state in mdp.states:\n",
    "        print('{:15} {:15} {:15}'.format(state, V.get(state, 0), pi.get(state, 'none')))\n",
    "\n",
    "    # Return the optimal value function and policy\n",
    "    return V, pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e85c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b2e5caaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s               V(s)            pi(s)          \n",
      "              1 -11.999999999905413 walk           \n",
      "              2 -10.999999999951797 walk           \n",
      "              3 -9.999999999975444 walk           \n",
      "              4 -8.999999999987494 walk           \n",
      "              5 -7.9999999999936335 tram           \n",
      "              6 -7.999999999998181 walk           \n",
      "              7 -6.9999999999990905 walk           \n",
      "              8 -5.999999999999545 walk           \n",
      "              9 -4.999999999999773 walk           \n",
      "             10 -3.9999999999998863 tram           \n",
      "             11            -9.0 walk           \n",
      "             12            -8.0 walk           \n",
      "             13            -7.0 walk           \n",
      "             14            -6.0 walk           \n",
      "             15            -5.0 walk           \n",
      "             16            -4.0 walk           \n",
      "             17            -3.0 walk           \n",
      "             18            -2.0 walk           \n",
      "             19            -1.0 walk           \n",
      "             20               0 none           \n"
     ]
    }
   ],
   "source": [
    "\n",
    "V_opt, optimal_policy = value_iteration_with_policy(TramMDP(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1f92b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
